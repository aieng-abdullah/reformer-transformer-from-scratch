<h1 align="center">
   Reformer Transformer â€” From Scratch
</h1>

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1200/1*Yz45yt-uqMPYxDGbZGavTw.png" width="60%">
</p>

<p align="center">
  <b>A PyTorch Reimplementation of the Reformer Architecture â€” With LSH Attention, Reversible Layers, and Chunked Feed-Forward Networks</b>
</p>

---

## ğŸš€ What is Reformer?

Reformer is a breakthrough transformer architecture designed for **scaling attention to ultra-long sequences** (e.g., 64K tokens), introduced by [Kitaev et al., 2020](https://arxiv.org/abs/2001.04451). It tackles the quadratic memory/time bottleneck of standard attention using smart architectural choices:

| âš™ï¸ Component                      | ğŸ’¡ Purpose                                                                 |
|----------------------------------|----------------------------------------------------------------------------|
| ğŸ”‘ **LSH Attention**             | Reduces complexity from O(nÂ²) â†’ O(n log n) via Locality-Sensitive Hashing |
| â™»ï¸ **Reversible Layers**         | Save GPU memory by recomputing instead of storing hidden states           |
| ğŸ§± **Chunked Feed-Forward**      | Breaks computation into chunks to reduce peak memory usage                |
| ğŸ“ **Axial Positional Encoding** | Enables long sequence encoding without massive memory use                 |

---

## ğŸ“œ Paper Reference

> **Title:** Reformer: The Efficient Transformer  
> **Authors:** Nikita Kitaev, Åukasz Kaiser, Anselm Levskaya  
> **Published in:** ICLR 2020  
> **Paper Link:** [arXiv:2001.04451](https://arxiv.org/abs/2001.04451)

---

## ğŸŒŸ Why This Project?

This is not just another implementation. This project aims to:

- ğŸ§  Teach the internals of Reformer, **line by line**
- ğŸ§± Provide a **modular**, clean PyTorch implementation
- ğŸ§ª Serve as a base for **research experiments**, **MLOps pipelines**, or **AI portfolio showcases**
- ğŸ“ Help **ML engineers**, **students**, and **researchers** understand memory-efficient Transformers

---

## ğŸ” Key Features

âœ… **Locality-Sensitive Hashing Attention**  
âœ… **Reversible Residual Connections**  
âœ… **Chunked Feed-Forward Network**  
âœ… **Axial Positional Encoding**  
âœ… **Full PyTorch implementation from scratch**  
âœ… **Clear comments, visualizations, and metrics tracking**  
âœ… **GPU-ready & Colab-compatible**

---

## âš¡ Technical Highlights

### 1. LSH Self-Attention
- Hashes similar query vectors into the same bucket
- Only computes attention within buckets
- Complexity reduced to O(n log n)

### 2. Reversible Residual Layers
- Forwards can be reversed at backprop time
- Saves memory (no activations need to be stored)

### 3. Chunked Feed-Forward
- Applies FFN block on chunks instead of full sequences
- Minimizes memory peak during training

### 4. Axial Positional Encoding
- Efficient 2D position embedding
- Works well with very long sequences (e.g., 4096+)

---

## ğŸ§  Who Should Use This?

- **ML Engineers** needing a scalable transformer model
- **Students** looking to understand Reformer deeply
- **Researchers** running experiments on long-sequence modeling
- **Hackers** and **AI tinkerers** wanting full control of attention

---

## ğŸ§° Tools & Frameworks

- **Python 3.10+**
- **PyTorch 2.x**
- **Matplotlib / Seaborn** (for visualizations)
- **Google Colab** (GPU supported)

---

## ğŸ“ Learn More

- [Reformer Explained (by Illustrated Transformer author)](https://theaisummer.com/reformer/)
- [LSH Attention Visualized (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)
- [Original Reformer GitHub (Google)](https://github.com/google/trax)

---

## ğŸ§© License

MIT License Â© 2025 â€” Built for educational, research, and experimentation purposes.

---

<p align="center">
  Built with ğŸ’¥ by an ML Engineer who believes in code that teaches.
</p>

ğŸ‘¨â€ğŸ’» Author
Abdullah Al Arif
[email:aieng.abdullah.arif@gmail.com]
